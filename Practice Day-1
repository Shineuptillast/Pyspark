from pyspark.conf import SparkConf
from pyspark.context import SparkContext
# Setting the Configuration for the spark application
conf = SparkConf()
conf.set('setMaster','local[3]')
conf.set('setAppName','Pyspark_app')
conf.set('spark.executor.memory','2g')
conf.setAll([('spark.driver.cores','4'),('spark.executors.cores','4')])

# Creating a spark context object using conf
sc = SparkContext.getOrCreate(conf=conf)

# Getting all the configurations using SparkContext object sc
sc._conf.getAll()

# Setting up some config properties using SparkContext object sc
sc._conf.set('spark.driver.memory','5g')

# Creating an RDD
rdd = sc.range(1,2000)
rdd.collect()

# Creating an rdd using range function with specified paritions
rdd1 = sc.range(1,20,2, numSlices=4)
rdd1.collect()





