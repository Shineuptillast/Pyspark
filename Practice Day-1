from pyspark.conf import SparkConf
from pyspark.context import SparkContext
# Setting the Configuration for the spark application
conf = SparkConf()
conf.set('setMaster','local[3]')
conf.set('setAppName','Pyspark_app')
conf.set('spark.executor.memory','2g')
conf.setAll([('spark.driver.cores','4'),('spark.executors.cores','4')])

# Creating a spark context object using conf
sc = SparkContext.getOrCreate(conf=conf)

# Getting all the configurations using SparkContext object sc
sc._conf.getAll()

# Setting up some config properties using SparkContext object sc
sc._conf.set('spark.driver.memory','5g')

# Creating an RDD 
# Will create an RDD of int Type
rdd = sc.range(1,2000)
rdd.collect()

# Creating an rdd using range function with specified paritions and step value
rdd1 = sc.range(1,20,2, numSlices=4)   # partition=4, Step_Size =2
rdd1.collect()
# Creating an rdd using range function with negative step value
rdd2 = sc.range(10,1,-2, numSlices=4)
rdd2.collect()

# Creating an RDD from Python Collection using parallelize function
This func only creates RDD from collection of python objects 
# From Python List
rdd3 = sc.parallelize([1,2,3,4,5,6,7,8,9,10]) # lIST OF INTEGER
rdd3.collect()

# From List of Dict 
dict_list = [ {"name": "Naman"},
              {"name":"Tejal"},
              {"name":"Preeti"}
]
rdd4 = sc.parallelize(dict_list)
rdd4.collect()

# collection can be list of any python datatype
# Data in collection is partitioned by the spark across the cluster nodes
# Number of partitions depend upon the resources available in the spark cluster to run spark application

# Manually Setting the partition parameter in parallelize method
rdd5 = sc.parallelize([True,False,True,True], numSlices=6)
rdd5.getNumPartitions()

# Creating an RDD from Textfile
# Create a text file in your local system
rdd6 = sc.textFile(path='/opt/spark/waor-dir/Folder/first_text.txt')
rdd6.collect()

# Manually adding the partitions in the RDD creation 
rdd7 = sc.textFile(path="/opt/spark/work-dir/Folder/first_text.txt", 10)
rdd7.getNumParitions()

# Cache the RDD 
# Its caching the RDD into Memory
rdd7 = sc.range(1,20)
rdd7.is_cached
# False Because it's hasnt been cached yet into Memory
rdd7.cache()
rdd.is_cached

# Persist is the other option to store the RDD into different StorageLevel 
from pyspark.storagelevel import Storagelevel

rdd8 = sc.parallelize([1,2,3,4,5],4)
rdd8.persist(StorageLevel.MEMORY_AND_DISK_2)
rdd8.is_cached
#True
print(str(rdd8.getStorageLevel()))









