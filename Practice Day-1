from pyspark.conf import SparkConf
from pyspark.context import SparkContext
# Setting the Configuration for the spark application
conf = SparkConf()
conf.set('setMaster','local[3]')
conf.set('setAppName','Pyspark_app')
conf.set('spark.executor.memory','2g')
conf.setAll([('spark.driver.cores','4'),('spark.executors.cores','4')])

# Creating a spark context object using conf
sc = SparkContext.getOrCreate(conf=conf)

# Getting all the configurations using SparkContext object sc
sc._conf.getAll()

# Setting up some config properties using SparkContext object sc
sc._conf.set('spark.driver.memory','5g')

# Creating an RDD
rdd = sc.range(1,2000)
rdd.collect()

# Creating an rdd using range function with specified paritions and step value
rdd1 = sc.range(1,20,2, numSlices=4)
rdd1.collect()
# Creating an rdd using range function with negative step value
rdd2 = sc.range(10,1,-2, numSlices=4)
rdd2.collect()

# Creating an RDD from Python Collection using parallelize function
# From Python List
rdd3 = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
rdd3.collect()

# From List of Dict 
dict_list = [ {"name": "Naman"},
              {"name":"Tejal"},
              {"name":"Preeti"}
]
rdd4 = sc.parallelize(dict_list)
rdd4.collect()

# From 


